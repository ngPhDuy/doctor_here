import os
import time
import logging
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain.schema import Document as LangChainDocument 
from docx import Document as DocxDocument
import requests

# ========== LOGGING ==========
logging.basicConfig(level=logging.INFO)
load_dotenv()

# ========== MODEL CONFIG ==========
LLAMA_MODEL_PATH = "models/t-llama-2-7b.Q4_K.gguf"
EMBEDDING_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
VECTORSTORE_PATH = os.path.join(BASE_DIR, "vectorstore", "faiss_index")
DATA_DIR = os.path.join(BASE_DIR, "med_data")

# ========== LOAD LLAMA ==========
# try:
#     logging.info(f"üîÅ Loading GGUF LLAMA model from {LLAMA_MODEL_PATH}...")
#     start = time.time()

#     model = Llama(
#         model_path=LLAMA_MODEL_PATH,
#         n_ctx=2048,
#         n_threads=multiprocessing.cpu_count(),  
#         n_gpu_layers=0,                    
#         verbose=False,
#     )

#     logging.info(f"‚úÖ GGUF LLAMA model loaded in {time.time() - start:.2f}s")
# except Exception as e:
#     logging.error(f"‚ùå Failed to load GGUF LLAMA model: {e}")
#     raise

# ========== LOAD VECTORSTORE + EMBEDDING ==========
try:
    logging.info("üîÅ Loading embedding model and FAISS vectorstore...")
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)
    vectorstore = FAISS.load_local(VECTORSTORE_PATH, embeddings, allow_dangerous_deserialization=True)

    ######### HYBRID RETRIEVER ######## 

    def load_docx_documents(data_dir):
        docs = []
        for filename in os.listdir(data_dir):
            if filename.endswith(".docx"):
                doc_path = os.path.join(data_dir, filename)
                doc = DocxDocument(doc_path)
                full_text = ""
                for para in doc.paragraphs:
                    full_text += para.text + "\n"
                docs.append(LangChainDocument(page_content=full_text))
        return docs

    docs = load_docx_documents(DATA_DIR)
    docs = [doc for doc in docs if doc.page_content.strip()]
    print(f"üìÑ Loaded {len(docs)} documents.")

    splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(docs)
    split_docs = [doc for doc in split_docs if doc.page_content.strip()]

    dense_retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

    bm25_retriever = BM25Retriever.from_documents(split_docs)
    bm25_retriever.k = 5

    hybrid_retriever = EnsembleRetriever(
        retrievers=[dense_retriever, bm25_retriever],
        weights=[0.5, 0.5],  # Try [0.7, 0.3] if semantic is better
    )
    logging.info("‚úÖ Vectorstore loaded and retriever initialized")
except Exception as e:
    logging.error(f"‚ùå Failed to load vectorstore: {e}")
    raise

# ========== BOT RESPONSE WITH RAG ==========

def ask_tllama(prompt: str, max_tokens: int = 512) -> str:

    payload = {
        "model": "gemma3:12b",
        "prompt": prompt,
        "stream": False
    }

    public_url = os.getenv("GEMMA_API_URL") 
    
    response = requests.post(f"{public_url}/api/generate", json=payload)
    response.raise_for_status() 

    data = response.json()

    return data.get("response", "") 

def get_bot_response(user_input: str) -> str:
    try:
        # 1. Retrieve related documents

        ###### HYBRID RETRIEVER ###### 

        retrieved_docs = hybrid_retriever.invoke(user_input)
        top_k_docs = retrieved_docs[:5]
        context = "\n\n".join([doc.page_content.strip()[:1000] for doc in top_k_docs])

        # 2. Compose prompt
        prompt = f"""B·∫°n l√† m·ªôt AI Assistant h·ªó tr·ª£ ng∆∞·ªùi d√πng s·ª≠ d·ª•ng ·ª©ng d·ª•ng chƒÉm s√≥c s·ª©c kh·ªèe "Doctor Here". H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, ch√≠nh x√°c v√† th√¢n thi·ªán. N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ t√≠nh nƒÉng, h∆∞·ªõng d·∫´n s·ª≠ d·ª•ng ho·∫∑c th·∫Øc m·∫Øc li√™n quan ƒë·∫øn b√°c sƒ©, l·ªãch h·∫πn, u·ªëng thu·ªëc, nh·∫Øc nh·ªü, k·∫øt qu·∫£ kh√°m... th√¨ h√£y tr·∫£ l·ªùi ƒë√∫ng theo t√†i li·ªáu h·ªá th·ªëng.

‚ùóKh√¥ng t·ª± suy ƒëo√°n th√¥ng tin ngo√†i t√†i li·ªáu. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y tr·∫£ l·ªùi: "Xin l·ªói, t√¥i ch∆∞a c√≥ th√¥ng tin v·ªÅ n·ªôi dung n√†y."

C·∫•u tr√∫c c√¢u tr·∫£ l·ªùi n√™n h∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc n·∫øu li√™n quan thao t√°c, ho·∫∑c m√¥ t·∫£ ng·∫Øn n·∫øu l√† gi·∫£i th√≠ch.

N·∫øu ng∆∞·ªùi d√πng ƒë·∫∑t c√°c c√¢u h·ªèi mang t√≠nh C√≥/Kh√¥ng (Yes/No) th√¨ h√£y tr·∫£ l·ªùi k√®m theo gi·∫£i th√≠ch ng·∫Øn g·ªçn.

D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• m·ªôt s·ªë c√¢u h·ªèi ng∆∞·ªùi d√πng c√≥ th·ªÉ h·ªèi b·∫°n:

1. ·ª®ng d·ª•ng Doctor Here d√πng ƒë·ªÉ l√†m g√¨?
2. L√†m sao ƒë·ªÉ t√¥i ƒë·∫∑t l·ªãch kh√°m v·ªõi b√°c sƒ©?
3. T√¥i c√≥ th·ªÉ chia s·∫ª th√¥ng tin kh√°m b·ªánh cho ng∆∞·ªùi th√¢n ƒë∆∞·ª£c kh√¥ng?
4. T√≠nh nƒÉng nh·∫Øc u·ªëng thu·ªëc ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?
5. T√¥i mu·ªën xem l·∫°i k·∫øt qu·∫£ kh√°m b·ªánh th√¨ v√†o ƒë√¢u?
6. C√≥ th·ªÉ nh·∫Øn tin v·ªõi b√°c sƒ© kh√¥ng?
7. N·∫øu h·∫øt th·ªùi gian t∆∞ v·∫•n, t√¥i c√≥ th·ªÉ gia h·∫°n ƒë∆∞·ª£c kh√¥ng?

Th√¥ng tin trong t√†i li·ªáu h∆∞·ªõng d·∫´n ƒë√£ bao g·ªìm to√†n b·ªô lu·ªìng thao t√°c ng∆∞·ªùi d√πng, v√¨ v·∫≠y h√£y tr·∫£ l·ªùi ch√≠nh x√°c theo n·ªôi dung t√†i li·ªáu n√†y.

Khi s·∫µn s√†ng, h√£y ch·ªù ng∆∞·ªùi d√πng ƒë·∫∑t c√¢u h·ªèi."

[Ng·ªØ c·∫£nh]
{context}

[C√¢u h·ªèi]
{user_input}

[Tr·∫£ l·ªùi]"""
        return prompt

        # # 3. Generate response using T-LLAMA GGUF model
        # logging.info("üß† Generating response from GGUF model...")
        # output = model(
        #     prompt,
        #     max_tokens=128,
        #     temperature=0.1,
        #     top_k=50,
        #     top_p=0.95,
        #     stop=["</s>", "###"],
        # )
        # response = output["choices"][0]["text"].strip()
        # return response

    except Exception as e:
        logging.error(f"‚ùå Error during RAG response: {e}")
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ x·ª≠ l√Ω c√¢u h·ªèi n√†y ngay l√∫c n√†y."